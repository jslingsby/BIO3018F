---
title: "" 
author: ""
format: 
  revealjs:
    theme: dark
    slide-number: true
    self-contained: true
editor: visual
---

# Power Analysis {background-color="#33431e"}

Jasper Slingsby

## Power Analysis {background-color="#33431e"}

<br>

No one ever does them...

<br>

...but they could save so much pain and suffering if they did!!!

## Power Analysis {background-color="#33431e"}

<br>

*Statistical power* is the probability of a hypothesis test finding an effect if there is an effect to be found.

<br>

*Power analysis* is a calculation typically used to estimate the smallest sample size needed for an experiment, given a required significance level, statistical power, and effect size.

-   It is *normally conducted before the data collection*!

## Why do power analysis? {background-color="#33431e"}

<br>

Firstly, it helps you plan your analyses before you've done your data collection, which is always useful.

<br>

Secondly, not knowing the statistical power of your analysis can result in

-   missed findings (through Type II Error), or
-   false findings (through Type I Error).

## Why do power analysis? {background-color="#33431e"}

<br>

Type II Error:

-   occurs when the researcher erroneously concludes that there [*is not*]{.underline} a difference between treatments, when in reality there is...
-   this is a common outcome of low statistical power

## Why do power analysis? {background-color="#33431e"}

<br>

Type I Error:

-   occurs when the researcher erroneously concludes that there [*is*]{.underline} a difference between treatments, when in reality there is not...
-   less likely when there is poor statistical power, but can happen with low sample sizes of highly variable subjects, or if there is bias in sampling...

## Why do power analysis? {background-color="#33431e"}

![Type I and Type II Errors and how they result in false or missing findings, respectively. Image from Norton and Strube 2001, *JOSPT*.](images/type_errors_Norton_Strube_2001.png){width="100%"}

## Statistical Power {background-color="#33431e"}

<br>

Is determined by the combination of the:

-   $\alpha$ ("significance") level required (e.g. P \< 0.05)
-   difference between group means (effect size)
-   variability among subjects
-   sample size (the factor we usually have most control over)

## $\alpha$ ("significance") level {.smaller background-color="#c2c190"}

```{r}
library(tidyverse)

ddat <- data.frame(x = rnorm(5000, 0, 1), Probability = rep("Normal Curve", 5000))

ggplot(ddat, aes(x, fill = Probability, colour = Probability)) +
  geom_density(alpha = 0.1, bw = 1) +
  theme(axis.text.x = element_blank()) +
  geom_segment(aes(x = 1.96, y = 0, xend = 1.96, yend = 0.3), col = "gray25", size = 0.5, alpha = .2, linetype = 3) +
  geom_segment(aes(x = -1.96, y = 0, xend = -1.96, yend = 0.3), col = "gray25", size = 0.5, alpha = .2, linetype = 3) +
  geom_segment(aes(x = -1.96, y = 0.1, xend = 1.96, yend = 0.1), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  annotate("text", x = 0, y = 0.125, label = "95% of samples") +
  annotate("text", x = 3, y = 0.125, label = "Upper 2.5%") +
  annotate("text", x = -3, y = 0.125, label = "Lower 2.5%") +
  ylab("Density") +
  xlim(-5,5)
```

We usually use an $\alpha$ of 0.05 to indicate significant difference.

-   i.e. the probability of the observation not being different to the null is less than 5% (i.e. p \< 0.05), or the result should only be observed once or less for every 20 samples.

This is a subjective cut-off, but is generally accepted in the literature...

## Difference between group means {.smaller background-color="#c2c190"}

```{r}
#library(tidyverse)

ddat <- data.frame(x = c(rnorm(5000, 0, 1), rnorm(5000, 0.5, 1), rnorm(5000, 5, 1)), Population = c(rep("P1", 5000), rep("P2", 5000), rep("P3", 5000)))

ggplot(ddat, aes(x, fill = Population, colour = Population)) +
  geom_density(alpha = 0.1, bw = 1) +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 0.3), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  geom_segment(aes(x = 0.5, y = 0, xend = 0.5, yend = 0.3), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 0.3), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  geom_segment(aes(x = 0, y = 0.1, xend = 5, yend = 0.1), col = "gray25", size = 0.5, alpha = .2, linetype = 3) +
  annotate("text", x = 2.5, y = 0.125, label = "Difference in means = 5") +
  geom_segment(aes(x = 0, y = 0.2, xend = 0.5, yend = 0.2), col = "gray25", size = 0.5, alpha = .2, linetype = 3) +
  annotate("text", x = 2.5, y = 0.225, label = "Difference in means = 0.5") +
  ylab("Density") +
  xlim(-5,10)
```

You have greater statistical power when you have greater differences in means (effect size). P1 vs P3 has greater power than either P1 vs P2 or P2 vs P3.

## Variability among subjects {.smaller background-color="#c2c190"}

```{r}
#| fig-height: 4

#library(tidyverse)

ddat <- rbind(data.frame(x = c(rnorm(5000, 0, 1), rnorm(5000, 0.5, 1), rnorm(5000, 5, 1)), Population = c(rep("P1", 5000), rep("P2", 5000), rep("P3", 5000)), variability = "SD = 1"), data.frame(x = c(rnorm(5000, 0, 2), rnorm(5000, 0.5, 2), rnorm(5000, 5, 2)), Population = c(rep("P1", 5000), rep("P2", 5000), rep("P3", 5000)), variability = "SD = 2"), data.frame(x = c(rnorm(5000, 0, 3), rnorm(5000, 0.5, 3), rnorm(5000, 5, 3)), Population = c(rep("P1", 5000), rep("P2", 5000), rep("P3", 5000)), variability = "SD = 3"))

ggplot(ddat, aes(x, fill = Population, colour = Population)) +
  geom_density(alpha = 0.1, bw = 1) +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 0.3), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  geom_segment(aes(x = 0.5, y = 0, xend = 0.5, yend = 0.3), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 0.3), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  ylab("Density") +
  xlim(-7.5,12.5)  +
  facet_wrap(vars(variability))
```
Greater variability among subjects results in larger standard deviations, reducing our ability to distinguish among groups (i.e. statistical power).

## Sample size {.smaller background-color="#c2c190"}

```{r}
#| fig-height: 4
#library(tidyverse)

ddat <- rbind(data.frame(x = c(rnorm(5000, 0, 1), rnorm(5000, 0.5, 1), rnorm(5000, 5, 1)), Population = c(rep("P1", 5000), rep("P2", 5000), rep("P3", 5000)), Sample = 5000), data.frame(x = c(rnorm(50, 0, 1), rnorm(50, 0.5, 1), rnorm(50, 5, 1)), Population = c(rep("P1", 50), rep("P2", 50), rep("P3", 50)), Sample = 50), data.frame(x = c(rnorm(5, 0, 1), rnorm(5, 0.5, 1), rnorm(5, 5, 1)), Population = c(rep("P1", 5), rep("P2", 5), rep("P3", 5)), Sample = 5))

ss <- ggplot(ddat, aes(x, fill = Population, colour = Population)) +
  geom_density(alpha = 0.1, bw = 1) +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 0.35), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  geom_segment(aes(x = 0.5, y = 0, xend = 0.5, yend = 0.35), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 0.35), col = "gray25", size = 0.5, alpha = .2, linetype = 5) +
  ylab("Density") +
  xlim(-5,10) +
  facet_wrap(vars(Sample))

ss_labels <- ddat %>% group_by(Sample, Population) %>% summarize(sd = sd(x)) %>% mutate(se = round(sd/sqrt(Sample), 3)) %>% mutate(label = paste0("SE = ", se))
  
p1_lab <- filter(ss_labels, Population == "P1")
p2_lab <- filter(ss_labels, Population == "P2")
p3_lab <- filter(ss_labels, Population == "P3")

ss +
  geom_text(x = -3.5, y = 0.25, aes(label = label), data = p1_lab, size = 3) +
  geom_text(x = -3.5, y = 0.2, aes(label = label), data = p2_lab, size = 3) +
  geom_text(x = -3.5, y = 0.15, aes(label = label), data = p3_lab, size = 3)
```

Increasing sample size increases statistical power by improving the estimate of the mean and constricting the distribution of the test statistic (i.e. reducing the standard error (SE)).

## How do we do power analysis? {background-color="#33431e"}

Simulate the data you would expect to collect, varying the:

-   $\alpha$ ("significance") level (e.g. P \< 0.05)
-   difference between group means (effect size)
-   variability among subjects
-   sample size (the factor we usually have most control over)

...and test for significant difference using the appropriate statistical test.

## Simulating data {background-color="#33431e"}

First, we need to simulate some data. 

If we believe our data are normally distributed, we can use the handy `rnorm()` function, like so:

```{r}
set.seed(10403)
```


```{r, echo = T}
dat <- rnorm(n = 50, # set the sample size
             mean = 1, # set the mean to = 1
             sd = 1) # set the standard deviation to = 1
```

Type `?Distributions` <Enter> into your R console for alternative distributions.

## Simulating data {background-color="#33431e"}

Now let's look at our new data

- This is easier if we make it a data frame

```{r, echo = T}

df <- data.frame(Data = dat, Treatment = 1)

head(df)
```

## Simulating data {background-color="#33431e"}

We can plot it like so:

```{r, echo = T}
ggplot(df, aes(Data, fill = Treatment, colour = Treatment)) +
  geom_density(alpha = 0.1)
```

## One sample _t_-test {.smaller background-color="#33431e"}

Tests the hypothesis that the mean of our population is a specific value (e.g. 0).

```{r, echo = T}
t.test(x = df$Data, # set our vector of data values
       alternative = "two.sided", # specify the alternative hypothesis (which in this case is "not zero" so it is two-sided (verses "greater" or "less"))
       mu = 0) # set the "true value" of the mean 
```

In this case, the difference is highly significant!
P < 0.00000005!!!

## One sample _t_-test {.smaller background-color="#33431e"}

What if we fiddle with the _**$\alpha$ ("significance" level)**_?

- You usually wouldn't do this!!! 

but 

- With one-sample _t_-tests one effectively does when choosing your alternative hypothesis.
  - We set it to be "two-sided" because our alternative was that the mean is "not zero". This means the result is only significantly different if the observed mean is in the upper or lower 2.5% of the distribution.
- If our alternative hypothesis was that the observed mean was "greater" or "less" then we could set that and the result would only be significantly different if the observed mean is in either the upper or lower 5% of the distribution respectively.
  - i.e. setting the alternative to "greater" or "less" effectively makes the test more sensitive, similar to increasing the $\alpha$
  
## One sample _t_-test {.smaller background-color="#33431e"}

Now let's fiddle with the difference between _**group means (effect size)**_.

In this case this is easiest done by shifting the _mu_ to closer to the mean of our randomly generated data, like so
```{r, echo = T}
t.test(x = df$Data, # set our vector of data values
       alternative = "two.sided", # specify the alternative hypothesis
       mu = 0.5) # set the "true value" of the mean 
```

Here we've reduced the effect size to from 1 to 0.5, but the result is still significantly different.

## One sample _t_-test {.smaller background-color="#33431e"}

Now let's fiddle with _**variability among subjects**_.

```{r, echo = T}
# Make new data with greater variability (standard deviation = 2)
df <- data.frame(Data = 
                  rnorm(n = 50, # set the sample size
                        mean = 1, # set the mean
                        sd = 2), # set bigger standard deviation
                 Treatment = 1)

# Run t-test
t.test(x = df$Data,
       alternative = "two.sided", 
       mu = 0.5)
```

With double the variability (standard deviation), and an effect size of 0.5, the result is no longer significantly different...

## One sample _t_-test {.smaller background-color="#33431e"}

Now let's increase the _**sample size**_.

```{r, echo = T}
# Make new data with greater sample size (n = 100)
df <- data.frame(Data = 
                  rnorm(n = 100, # set the sample size
                        mean = 1, # set the mean
                        sd = 2), # set bigger standard deviation
                 Treatment = 1)

# Run t-test
t.test(x = df$Data,
       alternative = "two.sided", 
       mu = 0.5)
```

Aha! By doubling our sample size, our result is significantly different once again...

## Estimating the number of samples {background-color="#33431e"}

<br>

Repeatedly rerunning our simulation with different sample size (_**n**_) would rapidly become tedious...

<br>

Fortunately, there's a better way!

<br>

_library(pwr)_ allows us to input the effect size and power required and returns the required _**n**_.

## Estimating the number of samples {.smaller background-color="#33431e"}

### One sample _t_-test

::: columns
::: {.column width="50%"}

```{r, echo = T}
library(pwr)

pwr.t.test(d = 0.8,
           n = NULL,
           sig.level = 0.05,
           power = 0.8,
           type = "one.sample", 
           alternative = "two.sided")
```

Which suggests we need 15 samples to achieve our desired statistical power.

:::
::: {.column width="50%"}
Here we have set _**n**_ = _NULL_ because that's the property we want to estimate.

_**power**_ = the power of the test (1 minus the Type II error probability), which in this case we have set to 80%

_**d**_ = Cohen's _d_ = a measure of  effect size = the difference between the means divided by the pooled standard deviation

  - i.e. you input the effect size and variability in one go

:::
:::

## Estimating the number of samples {.smaller background-color="#33431e"}

### Cohen's _d_

= the difference between the means divided by the pooled standard deviation (i.e. the standard deviation of the difference)

So, for our simulated data, assuming we're comparing our data to 0:

```{r, echo = T}
mean(df$Data)
```

```{r, echo = T}
sd(df$Data)
```

```{r, echo = T}
mean(df$Data)/sd(df$Data)
```

## Estimating the number of samples {.smaller background-color="#33431e"}

::: columns
::: {.column width="50%"}

Rerun for our observed _**d**_:

```{r, echo = T}
pwr.t.test(d = 0.65,
           n = NULL,
           sig.level = 0.05,
           power = 0.8,
           type = "one.sample", 
           alternative = "two.sided")
```

:::

::: {.column width="50%"}
And plotted:

```{r, echo = T}
pwr.t.test(d = 0.65,
           n = NULL,
           sig.level = 0.05,
           power = 0.8,
           type = "one.sample", 
           alternative = "two.sided") %>%
  plot()
```
:::
:::

## _**library(pwr)**_ functions: {.smaller background-color="#33431e"}

- `pwr.p.test`: one-sample proportion test
- `pwr.2p.test`: two-sample proportion test
- `pwr.2p2n.test`: two-sample proportion test (unequal sample sizes)
- `pwr.t.test`: two-sample, one-sample and paired t-tests
- `pwr.t2n.test`: two-sample t-tests (unequal sample sizes)
- `pwr.anova.test`: one-way balanced ANOVA
- `pwr.r.test`: correlation test
- `pwr.chisq.test`: chi-squared test (goodness of fit and association)
- `pwr.f2.test`: test for the general linear model

## _**library(pwr)**_ functions: {.smaller background-color="#33431e"}

<br>

### A note on effect sizes...

Each test has a different metric of effect size, each calculated in a different way...

Fortunately, _library(pwr)_ has a convenient function (`cohen.ES`) that can provide these for you for small, medium and large effect sizes.

```{r, echo = T}
cohen.ES(test = "t", size = "medium")
```

If you're not sure of the effect size you'd expect, the conservative approach is to use "small"

## _**library(pwr)**_ functions: {.smaller background-color="#33431e"}

<br>

### A note on effect sizes...

You can pass the results of `cohen.ES1` directly to the pwr function by calling the `effect.size` slot in the returned object:

```{r, echo = T}
str(cohen.ES(test = "t", size = "medium"))
```

```{r, echo = T}
cohen.ES(test = "t", size = "medium")$effect.size
```

## _**library(pwr)**_ functions: ANOVA {.smaller background-color="#c2c190"}

<br>

::: columns
::: {.column width="50%"}

For comparisons among 3 or more groups

```{r, echo = T}
pwr.anova.test(f = cohen.ES(test = "anov", size = "small")$effect.size,
               k = 4,
               power = 0.80,
               sig.level = 0.05) %>%
  plot
```

::: 
::: {.column width="50%"}

<br>

f	= Effect size


k	= Number of groups

```{r}
pwr.anova.test(f = cohen.ES(test = "anov", size = "large")$effect.size,
               k = 4,
               power = 0.80,
               sig.level = 0.05) %>%
  plot
```
::: 
::: 

## _**library(pwr)**_ functions: Chi-squared {.smaller background-color="#c2c190"}

<br>

::: columns
::: {.column width="50%"}

To test whether two categorical variables (two dimensions of a contingency table) are independent

```{r, echo = T}
pwr.chisq.test(w = cohen.ES(test = "chisq", size = "small")$effect.size,
              df = 1,
              power = 0.80,
              sig.level = 0.05) %>%
  plot()
```

::: 

::: {.column width="50%"}

<br>

w	= Effect size

<br>

df = Degrees of freedom

<br>

```{r}
pwr.chisq.test(w = cohen.ES(test = "chisq", size = "large")$effect.size,
              df = 1,
              power = 0.80,
              sig.level = 0.05) %>%
  plot()
```
::: 
::: 

## _**library(pwr)**_ functions: Correlation {.smaller background-color="#c2c190"}

<br>

::: columns
::: {.column width="50%"}

For correlation analysis

```{r, echo = T}
pwr.r.test(r = cohen.ES(test = "r", size = "small")$effect.size,
           power = 0.80,
           sig.level = 0.05,
           alternative = "two.sided") %>%
  plot()
```

::: 

::: {.column width="50%"}

r	= correlation coefficient (i.e. it is not $R^2$)

Note that for regression analysis you'd need to set the "alternative" to "greater" or "less", because it assumes that one variable is dependent on the other

```{r}
pwr.r.test(r = cohen.ES(test = "r", size = "large")$effect.size,
           power = 0.80,
           sig.level = 0.05,
           alternative = "two.sided") %>%
  plot()
```
::: 
::: 

## _**library(pwr)**_ functions: GLM {.smaller background-color="#c2c190"}

<br>

::: columns
::: {.column width="50%"}

To test whether two categorical variables (two dimensions of a contingency table) are independent

```{r, echo = T}
pwr.f2.test(u = 1,
            v = NULL,
            f2 = cohen.ES(test = "f2", size = "small")$effect.size,
            power = 0.8,
            sig.level = 0.05)
```

::: 

::: {.column width="50%"}

f2	= Effect size

u = degrees of freedom for numerator (= number of groups - 1)

v	= degrees of freedom for denominator (= total number of individuals across groups - the number of groups)

```{r}
pwr.f2.test(u = 1,
            v = NULL,
            f2 = cohen.ES(test = "f2", size = "large")$effect.size,
            power = 0.8,
            sig.level = 0.05)
```
::: 
::: 

## The plan from here... {.smaller background-color="#33431e"}

<br>

1. Think carefully about the data you plan to collect and how to analyze them

2. Decide on your statistical analyses/tests

3. Design your data spreadsheet

4. Design your field data sheet

5. Do power analyses for each of your analyses/tests to determine necessary sample sizes

6. If that all goes quickly, we could head into the field to collect some preliminary data